---
title: "Crossvalidating Schelling"
author: "Ernesto Carrella"
date: "February 2021"
output: 
  rmarkdown::html_vignette:
    fig_width: 8
    fig_height: 6
vignette: >
  %\VignetteIndexEntry{Crossvalidating Schelling}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  cache.extra = knitr::rand_seed,
  warning=FALSE,
  message=FALSE,
  comment = "#>"
)
set.seed(0)

```

## What's this?

The gist of this package is to quickly estimate simulation parameters from data using "*reference table*" methods i.e.

1. Run the model a bunch of times feeding it random parameters
2. Collect for each set of parameters you put in, a set of summary statistics describing what the model outputs
3. Try to generalize this data-set of input-output simulations via a regression/run an Approximate Bayesian Computaiton
4. Use the generalization to estimate the REAL parameters observing REAL summary statistics.

It's an **extremely inefficient** way of estimating models compared to search-based methods but it is has also many advantages:

1. It produces decent **confidence intervals**
2. It is **quickly testable** (figure out how good the estimates/CI are) 
3. It's all "post-processing": all it needs is a csv files with a bunch of random runs, no need to mess around with NETLOGO or whatever else you are using to run the model

I go on and on in the [paper](https://carrknight.github.io/assets/nofreelunch.html) on how these features are far more important than efficiency so let's try to explain why.

## Estimating Schelling with ONE summary statistics

We are studying the housing spatial distribution of 4 racial groups within a city and we notice a steady trend towards segregation. We think the Schelling segregation model could provide a good explanation for it so we want to estimate its parameters given the data we have.  
Unfortunately the only data we have is that "similarity" (% of people of the same race being neighbors, i.e. segregation) has increased steadily, at a linear trend of about .1 per month.

So what we do is that we take a [NETLOGO version](https://www.comses.net/codebases/7c562b23-4964-4d28-862c-1c8b254fd6ad/releases/1.0.0/) of the famous Schelling segregation model and just run it a bunch of times with random parameters.  
This package comes with those runs already done so let's load them!
```{r}
library(tidyverse)
library(ggplot2)
library(freelunch)

data("shelling")
glimpse(shelling)
```
We ran the model 5,000 times changing its three parameters `density`(controls how much "free" space is there for people to move about; set to between 50 and 99), `radiusNeighborhood`(how large spatially people look when judging their neighbors; between 1 and 5) and `X..similar.wanted` (% of similiar people wanted in one's neigbhorhood, between 25% and 75%). Besides the `random-seed` we collected 77 summary statistics (basic min/max/trend and snapshots for a few time series).

Of course in reality as we said we only have one summary statistic: `trend.percent.similar` is 0.1.  
We can try to estimate the three unknown Schelling parameters with that one summary statistic we have. Let's try a simple rejection ABC
```{r, message=FALSE}
estimation<-freelunch::fit_rejection_abc(training_runs = shelling,
                               target_runs = c(0.1),
                               parameter_colnames = c("density","radiusNeighborhood","X..similar.wanted"),
                               summary_statistics_colnames = c("trend.percent.similar")
)
freelunch::tidy_up_estimation(estimation) %>% knitr::kable()
```
Well, we do get an estimate for the three parameters but accepting it at face value is very dangerous. Is it a good estimate?  
The first alarm bell starts ringing when we look at the confidence intervals: they are huge! Not much different from our original bound, in fact.

In practice however wide CIs alone are just a hint (and they could be wrong too!). If we want to know how good our estimate are we need to test the estimation method itself. We do this by cross-validation (taking some runs and pretending it's the real data to see if we estimate their parameters correctly).  
We can do this in one line here:
```{r, message=FALSE}
 abc_cv<-cross_validate_rejection_abc(total_data  = shelling,
                               ngroup = 5,
                               parameter_colnames = c("density","radiusNeighborhood","X..similar.wanted"),
                               summary_statistics_colnames = c("trend.percent.similar")
)
abc_cv$performance
```

And the results are awful.
Performance, defined as 1 minus the ratio of mean square root error of the method (abc here) and just guessing the average, goes from zero (the estimation is no better than just guessing the average) to 1 (perfect estimation).
Performance of zero means that both radius and %-similarity are not identified at all. Density has also abysmal performance. My hand rule is that things start looking somewhat identified when performance reaches 0.3; however in practice it's just much easier to just check visually what we have managed to estimate. 


The confidence intervals seem to be a bit too large since their coverage (the % of time the real parameter is in the CI) exceeds the target of 95%:
```{r}
abc_cv$contained
```

and we can confirm this visually::
```{r}
plot_confidence_intervals(abc_cv,limit_number_of_runs_to_plot_to = 60) +
  theme_minimal()
```

Red dots are the "real" value and the error bars are the CI of our estimations. Again, it's clear we haven't really discovered anything: the CIs are very wide and about the size of the original priors. ABC is refusing to make a call.    
Notice however that it seems to be possible to narrow down the % similarity wanted parameter when it's around 40%-65%. This makes sense since that's the sweet-spot of Schelling (much lower nobody moves, much higher everybody is always dissatisfied and moves).

We can look also at point-prediction quality, which is just as dreadful
```{r}
plot_point_prediction_quality(abc_cv) 
```

We would like points to stay on the 45 degree line and instead they are everywhere. I think it's clear the estimation here doesn't work.  
The model is **under-identified** with the data we have!

We could try with something a bit more fancy, like a local-linear ABC but it ends up being no better (in fact slightly worse!):
```{r, message=FALSE }
ll_cv<-cross_validate_loclinear_abc(total_data  = shelling,
                               ngroup = 5,
                               parameter_colnames = c("density","radiusNeighborhood","X..similar.wanted"),
                               summary_statistics_colnames = c("trend.percent.similar")
)
ll_cv$performance
```

We could (should) try other methods but it it seems that you are just unable to estimate those parameters with just one summary statistic.


## Adding more summary statistics

### Trend in general unhappiness

So we couldn't quite estimate anything just looking at the segregation trend.  What if we tried adding more data?

Imagine that we rummage through the real data and discover that together with a trend in segregation, somebody ran a monthly survey asking people if they were unhappy about their neighbors and were planning to move. This nets us a second trend (`trend.percent.unhappy`) and we can plug it in to see the effects.

So we run again the cross-validation with one more summary statistic, this time using GAM (because we are fancy and probably listen to radio 4 on our way to work).
```{r, message=FALSE}
gam_cv<-cross_validate_gam(shelling,ngroup=5,
                             parameter_colnames = c("density","radiusNeighborhood","X..similar.wanted"),
                             summary_statistics_colnames =
                                 c(
                                   #new summary statistic:
                                   "trend.percent.unhappy",
                                   "trend.percent.similar"))
gam_cv$performance
```
Unfortunately performance is still very low, and it's clear by looking at the plots that we are still far from properly estimating the model.  

```{r}
gam_cv %>% plot_confidence_intervals()
```


### Trend in general unhappiness - per population

We go back to the data and see that we can  disaggregate the overall trend in unhappiness and compute it separately for two of the four sub-populations: the "red" population  (which is the majority) and "blue" population (which is one of three minorities).  

This leaves us with four summary statistics.
Let's switch to random forests for this, maybe using the "quick and dirty" option to run it quickly:
```{r, message=FALSE}
gam_rf<-cross_validate_random_forest(shelling,ngroup=5, fast=TRUE,
                             parameter_colnames = 
                               c("density","radiusNeighborhood","X..similar.wanted"),
                             summary_statistics_colnames =
                              c("trend.percent.unhappy",
                                ##new summary statistics here:
                                "trend.percent.unhappy.Red",
                                "trend.percent.unhappy.Blue",
                                 "trend.percent.similar"))

gam_rf$performance
```

That performance is much better, particularly for "% of similarity wanted"! And the visuals confirm it.
```{r}
gam_rf %>% plot_confidence_intervals()
```
```{r}
gam_rf %>% plot_point_prediction_quality()
```

### Now we can estimate!

So now that we have some confidence in the estimation capability of the model given the data we have, we can plug in the "real" values


```{r message=FALSE}
 estimation<-fit_random_forest(training_runs = shelling, fast=TRUE,
                               target_runs = 
                                 ### pluggin in the "real" data:
                                 c(
                                               -0.02,
                                               -0.05,
                                               -0.02,
                                               0.1),
                               parameter_colnames = c("density","radiusNeighborhood","X..similar.wanted"),
                             summary_statistics_colnames =
                              c("trend.percent.unhappy",
                                ##new summary statistics here:
                                "trend.percent.unhappy.Red",
                                "trend.percent.unhappy.Blue",
                                 "trend.percent.similar"))
tidy_up_estimation(estimation) %>% knitr::kable()

```

So it is still quite difficult to get neighborhood and density narrowed down but at least we know that "% of similarity wanted" is somewhere between 56% and 68%; the scientific proof that people are quite racist. Time to tweet the results and harvest the likes.

### What next?

Well, this really depends. We now have a parametrized model, we could go back to the simulator, plug in the parameters and compute alternative histories, policies and so on.  
It might be that it's imperative to get the `radiusNeighborhood` right, at which point we need to come back here and try to narrow down its range and improve its performance. But at least this package can help.
